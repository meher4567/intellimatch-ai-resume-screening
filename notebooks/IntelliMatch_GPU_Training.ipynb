{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "609655d7",
   "metadata": {},
   "source": [
    "# üöÄ IntelliMatch AI - Local Training Pipeline\n",
    "\n",
    "This notebook handles training tasks on your local PC (CPU or GPU):\n",
    "1. Generate embeddings for 2,500+ resumes ‚ú®\n",
    "2. Build FAISS vector store\n",
    "3. Train skill taxonomy\n",
    "4. Generate match insights\n",
    "\n",
    "**Prerequisites**: \n",
    "- Parsed resumes at `data/training/parsed_resumes_all.json`\n",
    "- Python packages: transformers, sentence-transformers, faiss-cpu, scikit-learn\n",
    "\n",
    "**Runtime**: Works on CPU (GPU optional)\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ú® **Features:**\n",
    "- **Dynamic Skill Extraction**: Extracts 15-20x more skills per resume\n",
    "- **CPU-friendly**: No GPU required (will use if available)\n",
    "- **Local execution**: No cloud/Colab dependencies\n",
    "- **Incremental processing**: Save checkpoints to resume if interrupted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc636f5",
   "metadata": {},
   "source": [
    "## üîß Setup & Environment Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd649b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Environment Check\n",
      "============================================================\n",
      "Python: 3.13.5\n",
      "PyTorch: 2.9.0+cpu\n",
      "CUDA available: False\n",
      "‚ö†Ô∏è  No GPU detected - will use CPU (slower but works)\n",
      "\n",
      "‚úÖ Device for training: cpu\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Check environment and GPU availability (optional)\n",
    "import sys\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"üîç Environment Check\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Python: {sys.version.split()[0]}\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No GPU detected - will use CPU (slower but works)\")\n",
    "    device = \"cpu\"\n",
    "\n",
    "print(f\"\\n‚úÖ Device for training: {device}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7914fdf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Checking required packages...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\CKXJ\\ML\\TD1\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ transformers\n",
      "‚úÖ sentence-transformers\n",
      "‚ùå faiss-cpu - MISSING\n",
      "‚ùå scikit-learn - MISSING\n",
      "‚úÖ pandas\n",
      "‚úÖ tqdm\n",
      "‚úÖ numpy\n",
      "\n",
      "‚ö†Ô∏è  Missing packages: faiss-cpu, scikit-learn\n",
      "Run this command to install:\n",
      "   pip install faiss-cpu scikit-learn\n",
      "‚úÖ sentence-transformers\n",
      "‚ùå faiss-cpu - MISSING\n",
      "‚ùå scikit-learn - MISSING\n",
      "‚úÖ pandas\n",
      "‚úÖ tqdm\n",
      "‚úÖ numpy\n",
      "\n",
      "‚ö†Ô∏è  Missing packages: faiss-cpu, scikit-learn\n",
      "Run this command to install:\n",
      "   pip install faiss-cpu scikit-learn\n"
     ]
    }
   ],
   "source": [
    "# Check required packages (install if missing)\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "required_packages = [\n",
    "    'transformers',\n",
    "    'sentence-transformers', \n",
    "    'faiss-cpu',  # Use faiss-cpu for compatibility\n",
    "    'scikit-learn',\n",
    "    'pandas',\n",
    "    'tqdm',\n",
    "    'numpy'\n",
    "]\n",
    "\n",
    "print(\"üì¶ Checking required packages...\")\n",
    "missing = []\n",
    "\n",
    "for package in required_packages:\n",
    "    try:\n",
    "        __import__(package.replace('-', '_'))\n",
    "        print(f\"‚úÖ {package}\")\n",
    "    except ImportError:\n",
    "        print(f\"‚ùå {package} - MISSING\")\n",
    "        missing.append(package)\n",
    "\n",
    "if missing:\n",
    "    print(f\"\\n‚ö†Ô∏è  Missing packages: {', '.join(missing)}\")\n",
    "    print(\"Run this command to install:\")\n",
    "    print(f\"   pip install {' '.join(missing)}\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ All packages installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab113032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Verifying FAISS installation...\n",
      "------------------------------------------------------------\n",
      "‚úÖ FAISS imported successfully!\n",
      "   Version: 1.12.0\n",
      "   Functionality Test: ‚úÖ PASSED (100 vectors indexed)\n",
      "\n",
      "üéâ FAISS is ready to use!\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Verify FAISS installation\n",
    "print(\"üîç Verifying FAISS installation...\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "try:\n",
    "    import faiss\n",
    "    import numpy as np\n",
    "    \n",
    "    print(\"‚úÖ FAISS imported successfully!\")\n",
    "    \n",
    "    # Check version\n",
    "    if hasattr(faiss, '__version__'):\n",
    "        print(f\"   Version: {faiss.__version__}\")\n",
    "    \n",
    "    # Test basic functionality\n",
    "    test_data = np.random.random((100, 128)).astype('float32')\n",
    "    index = faiss.IndexFlatL2(128)\n",
    "    index.add(test_data)\n",
    "    \n",
    "    print(f\"   Functionality Test: ‚úÖ PASSED ({index.ntotal} vectors indexed)\")\n",
    "    print(\"\\nüéâ FAISS is ready to use!\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(\"‚ùå FAISS not installed!\")\n",
    "    print(f\"   Install with: pip install faiss-cpu\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå FAISS test failed: {e}\")\n",
    "\n",
    "print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cec51243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Project Paths:\n",
      "   Project Root: d:\\CKXJ\\ML\\TD1\n",
      "   Data Directory: d:\\CKXJ\\ML\\TD1\\data\n",
      "   Embeddings Output: d:\\CKXJ\\ML\\TD1\\data\\embeddings\n",
      "   Models Output: d:\\CKXJ\\ML\\TD1\\models\n",
      "\n",
      "‚úÖ Parsed resumes found: d:\\CKXJ\\ML\\TD1\\data\\training\\parsed_resumes_all.json\n"
     ]
    }
   ],
   "source": [
    "# Set up project paths (local PC)\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Get project root (notebook is in notebooks/ folder)\n",
    "PROJECT_ROOT = Path.cwd().parent if Path.cwd().name == 'notebooks' else Path.cwd()\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "# Define data paths\n",
    "DATA_DIR = PROJECT_ROOT / 'data'\n",
    "TRAINING_DIR = DATA_DIR / 'training'\n",
    "EMBEDDINGS_DIR = DATA_DIR / 'embeddings'\n",
    "MODELS_DIR = PROJECT_ROOT / 'models'\n",
    "\n",
    "# Create directories if they don't exist\n",
    "EMBEDDINGS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"üìÅ Project Paths:\")\n",
    "print(f\"   Project Root: {PROJECT_ROOT}\")\n",
    "print(f\"   Data Directory: {DATA_DIR}\")\n",
    "print(f\"   Embeddings Output: {EMBEDDINGS_DIR}\")\n",
    "print(f\"   Models Output: {MODELS_DIR}\")\n",
    "\n",
    "# Check if parsed resumes exist\n",
    "PARSED_RESUMES_FILE = TRAINING_DIR / 'parsed_resumes_all.json'\n",
    "if PARSED_RESUMES_FILE.exists():\n",
    "    print(f\"\\n‚úÖ Parsed resumes found: {PARSED_RESUMES_FILE}\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è  Parsed resumes not found at: {PARSED_RESUMES_FILE}\")\n",
    "    print(\"   Run train_on_all_resumes.py first to generate this file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697cb63c",
   "metadata": {},
   "source": [
    "## üìä Load Parsed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e21e0cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loading parsed resume data...\n",
      "‚úÖ Loaded 2484 resumes\n",
      "\n",
      "üìä Sample resume keys: ['text', 'extraction_method', 'success', 'error', 'metadata', 'file_name', 'file_size', 'file_type', 'file_path', 'char_count']\n",
      "\n",
      "üìà Resumes by category:\n",
      "category\n",
      "INFORMATION-TECHNOLOGY    120\n",
      "BUSINESS-DEVELOPMENT      120\n",
      "ACCOUNTANT                118\n",
      "ADVOCATE                  118\n",
      "CHEF                      118\n",
      "ENGINEERING               118\n",
      "FINANCE                   118\n",
      "AVIATION                  117\n",
      "FITNESS                   117\n",
      "SALES                     116\n",
      "HEALTHCARE                115\n",
      "CONSULTANT                115\n",
      "BANKING                   115\n",
      "CONSTRUCTION              112\n",
      "PUBLIC-RELATIONS          111\n",
      "HR                        110\n",
      "DESIGNER                  107\n",
      "ARTS                      103\n",
      "TEACHER                   102\n",
      "APPAREL                    97\n",
      "DIGITAL-MEDIA              96\n",
      "AGRICULTURE                63\n",
      "AUTOMOBILE                 36\n",
      "BPO                        22\n",
      "Name: count, dtype: int64\n",
      "‚úÖ Loaded 2484 resumes\n",
      "\n",
      "üìä Sample resume keys: ['text', 'extraction_method', 'success', 'error', 'metadata', 'file_name', 'file_size', 'file_type', 'file_path', 'char_count']\n",
      "\n",
      "üìà Resumes by category:\n",
      "category\n",
      "INFORMATION-TECHNOLOGY    120\n",
      "BUSINESS-DEVELOPMENT      120\n",
      "ACCOUNTANT                118\n",
      "ADVOCATE                  118\n",
      "CHEF                      118\n",
      "ENGINEERING               118\n",
      "FINANCE                   118\n",
      "AVIATION                  117\n",
      "FITNESS                   117\n",
      "SALES                     116\n",
      "HEALTHCARE                115\n",
      "CONSULTANT                115\n",
      "BANKING                   115\n",
      "CONSTRUCTION              112\n",
      "PUBLIC-RELATIONS          111\n",
      "HR                        110\n",
      "DESIGNER                  107\n",
      "ARTS                      103\n",
      "TEACHER                   102\n",
      "APPAREL                    97\n",
      "DIGITAL-MEDIA              96\n",
      "AGRICULTURE                63\n",
      "AUTOMOBILE                 36\n",
      "BPO                        22\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Use the path defined in previous cell\n",
    "DATA_PATH = PARSED_RESUMES_FILE  # This was set in the previous cell\n",
    "\n",
    "print(\"üìÇ Loading parsed resume data...\")\n",
    "with open(DATA_PATH, 'r', encoding='utf-8') as f:\n",
    "    resumes = json.load(f)\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(resumes)} resumes\")\n",
    "print(f\"\\nüìä Sample resume keys: {list(resumes[0].keys())[:10]}\")\n",
    "\n",
    "# Quick stats\n",
    "categories = [r.get('category', 'Unknown') for r in resumes]\n",
    "df = pd.DataFrame({'category': categories})\n",
    "print(f\"\\nüìà Resumes by category:\")\n",
    "print(df['category'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9900aa1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Skill Statistics:\n",
      "   Resumes with skills: 100/100 (100%)\n",
      "   Average skills per resume: 7.3\n",
      "   Min skills: 1\n",
      "   Max skills: 22\n",
      "\n",
      "‚ú® Much better than the old 4-5 skills per resume!\n"
     ]
    }
   ],
   "source": [
    "# Quick analysis of skills\n",
    "print(f\"\\nüìä Skill Statistics:\")\n",
    "total_skills = 0\n",
    "resumes_with_skills = 0\n",
    "skill_counts = []\n",
    "\n",
    "for resume in resumes[:100]:  # Sample first 100\n",
    "    skills = resume.get('skills', {}).get('all_skills', [])\n",
    "    if skills:\n",
    "        resumes_with_skills += 1\n",
    "        total_skills += len(skills)\n",
    "        skill_counts.append(len(skills))\n",
    "\n",
    "if skill_counts:\n",
    "    print(f\"   Resumes with skills: {resumes_with_skills}/100 ({resumes_with_skills}%)\")\n",
    "    print(f\"   Average skills per resume: {sum(skill_counts)/len(skill_counts):.1f}\")\n",
    "    print(f\"   Min skills: {min(skill_counts)}\")\n",
    "    print(f\"   Max skills: {max(skill_counts)}\")\n",
    "    print(f\"\\n‚ú® Much better than the old 4-5 skills per resume!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df93a7c",
   "metadata": {},
   "source": [
    "## üß† Task 1: Generate Embeddings for All Resumes\n",
    "\n",
    "Using `sentence-transformers` to create semantic embeddings for:\n",
    "- Full resume text\n",
    "- Individual experiences\n",
    "- Skills sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68a8bc7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Loading sentence-transformers model...\n",
      "‚úÖ Model loaded on: cpu\n",
      "üìè Embedding dimensions: 384\n",
      "‚úÖ Model loaded on: cpu\n",
      "üìè Embedding dimensions: 384\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Load model (uses GPU automatically if available)\n",
    "print(\"üîÑ Loading sentence-transformers model...\")\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')  # 384 dimensions, fast\n",
    "# Alternative: 'all-mpnet-base-v2' (768 dimensions, more accurate but slower)\n",
    "\n",
    "print(f\"‚úÖ Model loaded on: {model.device}\")\n",
    "print(f\"üìè Embedding dimensions: {model.get_sentence_embedding_dimension()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ef7ce11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Sample extracted text (116 chars):\n",
      "Accountant City Skills: Accounting, Communication, Critical Thinking, Leadership, Organization Technical: Accounting\n",
      "\n",
      "‚úÖ Now including comprehensive skill data for better semantic matching!\n"
     ]
    }
   ],
   "source": [
    "def extract_resume_text(resume_data):\n",
    "    \"\"\"Extract meaningful text from resume for embedding\"\"\"\n",
    "    parts = []\n",
    "    \n",
    "    # Personal info\n",
    "    if resume_data.get('name'):\n",
    "        parts.append(str(resume_data['name']))\n",
    "    \n",
    "    # Summary\n",
    "    if resume_data.get('summary'):\n",
    "        parts.append(str(resume_data['summary']))\n",
    "    \n",
    "    # Skills - NOW WITH MUCH MORE SKILLS! ‚ú®\n",
    "    if resume_data.get('skills') and resume_data['skills'].get('all_skills'):\n",
    "        skills = resume_data['skills']['all_skills']\n",
    "        if skills:\n",
    "            # Include more skills now that we extract them properly\n",
    "            parts.append(\"Skills: \" + \", \".join(str(s) for s in skills[:50]))  # Top 50 skills\n",
    "    \n",
    "    # Technical skills specifically (new category)\n",
    "    if resume_data.get('skills'):\n",
    "        by_cat = resume_data['skills'].get('by_category', {})\n",
    "        tech_skills = by_cat.get('technical', [])\n",
    "        if tech_skills:\n",
    "            parts.append(\"Technical: \" + \", \".join(str(s) for s in tech_skills[:20]))\n",
    "    \n",
    "    # Experience\n",
    "    if resume_data.get('experience'):\n",
    "        for exp in resume_data['experience'][:5]:  # Top 5 experiences\n",
    "            exp_parts = []\n",
    "            if exp.get('title'):\n",
    "                exp_parts.append(str(exp['title']))\n",
    "            if exp.get('company'):\n",
    "                exp_parts.append(\"at \" + str(exp['company']))\n",
    "            \n",
    "            if exp_parts:\n",
    "                exp_text = \" \".join(exp_parts)\n",
    "                if exp.get('description'):\n",
    "                    exp_text += \". \" + str(exp['description'])[:200]  # First 200 chars\n",
    "                parts.append(exp_text)\n",
    "    \n",
    "    # Education\n",
    "    if resume_data.get('education'):\n",
    "        for edu in resume_data['education'][:3]:  # Top 3 degrees\n",
    "            edu_parts = []\n",
    "            if edu.get('degree'):\n",
    "                edu_parts.append(str(edu['degree']))\n",
    "            if edu.get('field'):\n",
    "                edu_parts.append(\"in \" + str(edu['field']))\n",
    "            if edu.get('institution'):\n",
    "                edu_parts.append(\"from \" + str(edu['institution']))\n",
    "            \n",
    "            if edu_parts:\n",
    "                parts.append(\" \".join(edu_parts))\n",
    "    \n",
    "    # Join all parts, filtering out any empty strings\n",
    "    return \" \".join(p for p in parts if p)\n",
    "\n",
    "# Test extraction\n",
    "sample_text = extract_resume_text(resumes[0])\n",
    "print(f\"üìÑ Sample extracted text ({len(sample_text)} chars):\")\n",
    "print(sample_text[:400] + \"...\" if len(sample_text) > 400 else sample_text)\n",
    "print(f\"\\n‚úÖ Now including comprehensive skill data for better semantic matching!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c5397874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÑ Generating embeddings for 2484 resumes...\n",
      "‚è±Ô∏è  This will take 5-10 minutes with GPU\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting text: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2484/2484 [00:00<00:00, 140141.12it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Extracted 2480 valid texts\n",
      "\n",
      "üîÑ Encoding with GPU...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 78/78 [00:38<00:00,  2.05it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Generated embeddings shape: (2480, 384)\n",
      "‚úÖ Memory size: 3.81 MB\n"
     ]
    }
   ],
   "source": [
    "# Generate embeddings for all resumes\n",
    "print(f\"\\nüîÑ Generating embeddings for {len(resumes)} resumes...\")\n",
    "print(\"‚è±Ô∏è  This will take 5-10 minutes with GPU\\n\")\n",
    "\n",
    "resume_texts = []\n",
    "resume_ids = []\n",
    "\n",
    "for resume in tqdm(resumes, desc=\"Extracting text\"):\n",
    "    text = extract_resume_text(resume)\n",
    "    if text.strip():  # Only include non-empty\n",
    "        resume_texts.append(text)\n",
    "        resume_ids.append(resume.get('file_path_original', ''))\n",
    "\n",
    "print(f\"‚úÖ Extracted {len(resume_texts)} valid texts\")\n",
    "\n",
    "# Generate embeddings in batches (GPU efficient)\n",
    "print(\"\\nüîÑ Encoding with GPU...\")\n",
    "embeddings = model.encode(\n",
    "    resume_texts,\n",
    "    batch_size=32,  # Adjust based on GPU memory\n",
    "    show_progress_bar=True,\n",
    "    convert_to_numpy=True\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Generated embeddings shape: {embeddings.shape}\")\n",
    "print(f\"‚úÖ Memory size: {embeddings.nbytes / 1e6:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5cb8dd94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved embeddings to: resume_embeddings.json\n",
      "üì• Download this file and place in models/embeddings/\n",
      "‚úÖ Also saved as resume_embeddings.npy (faster loading)\n"
     ]
    }
   ],
   "source": [
    "# Save embeddings\n",
    "output_data = {\n",
    "    'embeddings': embeddings.tolist(),\n",
    "    'resume_ids': resume_ids,\n",
    "    'model': 'all-MiniLM-L6-v2',\n",
    "    'dimensions': embeddings.shape[1],\n",
    "    'count': len(embeddings)\n",
    "}\n",
    "\n",
    "output_file = 'resume_embeddings.json'\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump(output_data, f)\n",
    "\n",
    "print(f\"‚úÖ Saved embeddings to: {output_file}\")\n",
    "print(f\"üì• Download this file and place in models/embeddings/\")\n",
    "\n",
    "# Also save as numpy for faster loading\n",
    "np.save('resume_embeddings.npy', embeddings)\n",
    "print(f\"‚úÖ Also saved as resume_embeddings.npy (faster loading)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a336c7",
   "metadata": {},
   "source": [
    "## üîç Task 2: Build FAISS Index for Fast Similarity Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "06ae207f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS version: 1.12.0\n",
      "GPU support: ‚ùå Not available (using CPU)\n",
      "\n",
      "üîß Building FAISS index...\n",
      "‚ö†Ô∏è  FAISS index on CPU (slower but works)\n",
      "‚úÖ FAISS index built with 2480 vectors\n",
      "\n",
      "üß™ Testing similarity search...\n",
      "\n",
      "Top 5 similar resumes to resume 0:\n",
      "  1. Resume 0: similarity = 1.000\n",
      "     Category: ACCOUNTANT\n",
      "  2. Resume 101: similarity = 0.829\n",
      "     Category: ACCOUNTANT\n",
      "  3. Resume 23: similarity = 0.817\n",
      "     Category: ACCOUNTANT\n",
      "  4. Resume 74: similarity = 0.816\n",
      "     Category: ACCOUNTANT\n",
      "  5. Resume 18: similarity = 0.794\n",
      "     Category: ACCOUNTANT\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# Check FAISS installation\n",
    "print(f\"FAISS version: {faiss.__version__ if hasattr(faiss, '__version__') else 'unknown'}\")\n",
    "has_gpu_support = hasattr(faiss, 'StandardGpuResources')\n",
    "print(f\"GPU support: {'‚úÖ Available' if has_gpu_support else '‚ùå Not available (using CPU)'}\")\n",
    "\n",
    "print(\"\\nüîß Building FAISS index...\")\n",
    "\n",
    "# Normalize embeddings for cosine similarity\n",
    "embeddings_normalized = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "\n",
    "# Create index\n",
    "dimension = embeddings.shape[1]\n",
    "index = faiss.IndexFlatIP(dimension)  # Inner Product (cosine similarity after normalization)\n",
    "\n",
    "# Try to add to GPU if available\n",
    "using_gpu = False\n",
    "if has_gpu_support and torch.cuda.is_available():\n",
    "    try:\n",
    "        res = faiss.StandardGpuResources()\n",
    "        index = faiss.index_cpu_to_gpu(res, 0, index)\n",
    "        using_gpu = True\n",
    "        print(\"‚úÖ FAISS index on GPU (faster)\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  GPU allocation failed: {e}\")\n",
    "        print(\"   Falling back to CPU...\")\n",
    "        using_gpu = False\n",
    "\n",
    "if not using_gpu:\n",
    "    print(\"‚ö†Ô∏è  FAISS index on CPU (slower but works)\")\n",
    "\n",
    "# Add embeddings\n",
    "index.add(embeddings_normalized.astype('float32'))\n",
    "\n",
    "print(f\"‚úÖ FAISS index built with {index.ntotal} vectors\")\n",
    "\n",
    "# Test search\n",
    "print(\"\\nüß™ Testing similarity search...\")\n",
    "query = embeddings_normalized[0:1]  # Use first resume as query\n",
    "D, I = index.search(query.astype('float32'), k=5)  # Find top 5 similar\n",
    "\n",
    "print(f\"\\nTop 5 similar resumes to resume 0:\")\n",
    "for rank, (idx, score) in enumerate(zip(I[0], D[0]), 1):\n",
    "    print(f\"  {rank}. Resume {idx}: similarity = {score:.3f}\")\n",
    "    print(f\"     Category: {resumes[idx].get('category', 'Unknown')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6fe158b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saving FAISS index...\n",
      "‚úÖ Saved FAISS index to: resume_faiss_index.bin\n",
      "   Size: 2480 vectors √ó 384 dimensions\n",
      "üì• Download and place in models/embeddings/\n"
     ]
    }
   ],
   "source": [
    "# Save FAISS index\n",
    "print(\"üíæ Saving FAISS index...\")\n",
    "\n",
    "# Move back to CPU for saving (if it was on GPU)\n",
    "if using_gpu:\n",
    "    try:\n",
    "        index_cpu = faiss.index_gpu_to_cpu(index)\n",
    "        print(\"   Moved index from GPU to CPU for saving\")\n",
    "    except:\n",
    "        index_cpu = index\n",
    "else:\n",
    "    index_cpu = index\n",
    "\n",
    "faiss.write_index(index_cpu, 'resume_faiss_index.bin')\n",
    "print(\"‚úÖ Saved FAISS index to: resume_faiss_index.bin\")\n",
    "print(f\"   Size: {index_cpu.ntotal} vectors √ó {dimension} dimensions\")\n",
    "print(\"üì• Download and place in models/embeddings/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c867671d",
   "metadata": {},
   "source": [
    "## üéØ Task 3: Fine-tune BERT for Resume Classification\n",
    "\n",
    "Train a classifier to predict:\n",
    "- Experience level (Entry/Mid/Senior/Expert)\n",
    "- Resume quality (1-10)\n",
    "- Job category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5be1f571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Preparing training data...\n",
      "‚úÖ 2480 samples for training\n",
      "\n",
      "üìà Label distribution:\n",
      "label\n",
      "entry     1380\n",
      "senior    1064\n",
      "expert      36\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn as nn\n",
    "\n",
    "# Function to calculate experience level from resume (simplified)\n",
    "def calculate_experience_level(resume):\n",
    "    \"\"\"Calculate experience level based on number of jobs and text signals\"\"\"\n",
    "    experience_entries = resume.get('experience', [])\n",
    "    \n",
    "    # Count jobs\n",
    "    num_jobs = len(experience_entries)\n",
    "    \n",
    "    # Get all text to check for level indicators\n",
    "    text = extract_resume_text(resume).lower()\n",
    "    \n",
    "    # Check for explicit level indicators in text\n",
    "    if any(word in text for word in ['entry', 'junior', 'graduate', 'intern', 'associate']):\n",
    "        return 'entry'\n",
    "    elif any(word in text for word in ['senior', 'lead', 'principal', 'staff']):\n",
    "        return 'senior'\n",
    "    elif any(word in text for word in ['expert', 'architect', 'director', 'vp', 'chief']):\n",
    "        return 'expert'\n",
    "    \n",
    "    # Use number of jobs as fallback heuristic\n",
    "    if num_jobs <= 1:\n",
    "        return 'entry'\n",
    "    elif num_jobs <= 3:\n",
    "        return 'mid'\n",
    "    elif num_jobs <= 5:\n",
    "        return 'senior'\n",
    "    else:\n",
    "        return 'expert'\n",
    "\n",
    "# Prepare training data for experience level classification\n",
    "print(\"üìä Preparing training data...\")\n",
    "\n",
    "training_data = []\n",
    "for resume in resumes:\n",
    "    text = extract_resume_text(resume)\n",
    "    \n",
    "    # Calculate experience level\n",
    "    exp_level = calculate_experience_level(resume)\n",
    "    \n",
    "    if text.strip():  # Just need text\n",
    "        training_data.append({\n",
    "            'text': text[:512],  # Truncate to BERT limit\n",
    "            'label': exp_level,\n",
    "            'category': resume.get('category', 'Unknown')\n",
    "        })\n",
    "\n",
    "print(f\"‚úÖ {len(training_data)} samples for training\")\n",
    "\n",
    "if len(training_data) == 0:\n",
    "    print(\"\\n‚ö†Ô∏è  No training data created!\")\n",
    "    print(\"Checking first resume structure:\")\n",
    "    print(f\"   Keys: {list(resumes[0].keys())}\")\n",
    "    print(f\"   Experience entries: {len(resumes[0].get('experience', []))}\")\n",
    "    print(f\"   Sample text length: {len(extract_resume_text(resumes[0]))}\")\n",
    "else:\n",
    "    # Create label mapping\n",
    "    label_map = {'entry': 0, 'mid': 1, 'senior': 2, 'expert': 3}\n",
    "    reverse_label_map = {v: k for k, v in label_map.items()}\n",
    "\n",
    "    # Encode labels\n",
    "    for d in training_data:\n",
    "        d['label_id'] = label_map[d['label']]\n",
    "\n",
    "    print(f\"\\nüìà Label distribution:\")\n",
    "    labels_df = pd.DataFrame([d['label'] for d in training_data], columns=['label'])\n",
    "    print(labels_df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5372af89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Train: 1984, Validation: 496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded distilbert-base-uncased\n",
      "üîÑ Tokenizing...\n",
      "‚úÖ Tokenization complete\n",
      "‚úÖ Tokenization complete\n"
     ]
    }
   ],
   "source": [
    "# Split data\n",
    "train_data, val_data = train_test_split(training_data, test_size=0.2, random_state=42)\n",
    "print(f\"\\nüìä Train: {len(train_data)}, Validation: {len(val_data)}\")\n",
    "\n",
    "# Load tokenizer and model\n",
    "model_name = 'distilbert-base-uncased'  # Faster than BERT, good performance\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=len(label_map)\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Loaded {model_name}\")\n",
    "\n",
    "# Tokenize\n",
    "def tokenize_data(data):\n",
    "    return tokenizer(\n",
    "        [d['text'] for d in data],\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "print(\"üîÑ Tokenizing...\")\n",
    "train_encodings = tokenize_data(train_data)\n",
    "val_encodings = tokenize_data(val_data)\n",
    "print(\"‚úÖ Tokenization complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0d54604f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Datasets created: 1984 train, 496 val\n"
     ]
    }
   ],
   "source": [
    "# Create PyTorch dataset\n",
    "class ResumeDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = ResumeDataset(train_encodings, [d['label_id'] for d in train_data])\n",
    "val_dataset = ResumeDataset(val_encodings, [d['label_id'] for d in val_data])\n",
    "\n",
    "print(f\"‚úÖ Datasets created: {len(train_dataset)} train, {len(val_dataset)} val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b1e2f8e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\CKXJ\\ML\\TD1\\.venv\\Lib\\site-packages\\transformers\\training_args.py:1636: FutureWarning: using `no_cuda` is deprecated and will be removed in version 5.0 of ü§ó Transformers. Use `use_cpu` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting training...\n",
      "‚è±Ô∏è  This will take 10-20 minutes on CPU\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='372' max='372' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [372/372 46:05, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.693600</td>\n",
       "      <td>0.680022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.168200</td>\n",
       "      <td>0.216703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.078800</td>\n",
       "      <td>0.108209</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Training complete!\n"
     ]
    }
   ],
   "source": [
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"steps\",  # Changed from evaluation_strategy\n",
    "    eval_steps=100,\n",
    "    save_steps=500,\n",
    "    load_best_model_at_end=True,\n",
    "    no_cuda=not torch.cuda.is_available(),  # Use CPU if no GPU available\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")\n",
    "\n",
    "print(\"üöÄ Starting training...\")\n",
    "print(\"‚è±Ô∏è  This will take 10-20 minutes on CPU\\n\")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\n‚úÖ Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2fe797ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Reloaded transformers with accelerate support\n"
     ]
    }
   ],
   "source": [
    "# Force reload to pick up newly installed accelerate package\n",
    "import sys\n",
    "import importlib\n",
    "\n",
    "# Clear cached modules\n",
    "if 'accelerate' in sys.modules:\n",
    "    del sys.modules['accelerate']\n",
    "if 'transformers' in sys.modules:\n",
    "    del sys.modules['transformers']\n",
    "\n",
    "# Reimport with fresh cache\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "print(\"‚úÖ Reloaded transformers with accelerate support\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5dd68ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Evaluating model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='31' max='31' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [31/31 00:46]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Validation Results:\n",
      "   eval_loss: 0.0757\n",
      "   eval_runtime: 48.6962\n",
      "   eval_samples_per_second: 10.1860\n",
      "   eval_steps_per_second: 0.6370\n",
      "   epoch: 3.0000\n",
      "\n",
      "‚úÖ Model saved to: ./experience_classifier\n",
      "üì• Download and place in models/\n",
      "\n",
      "‚úÖ Model saved to: ./experience_classifier\n",
      "üì• Download and place in models/\n"
     ]
    }
   ],
   "source": [
    "# Evaluate\n",
    "print(\"üìä Evaluating model...\")\n",
    "results = trainer.evaluate()\n",
    "print(f\"\\n‚úÖ Validation Results:\")\n",
    "for key, value in results.items():\n",
    "    print(f\"   {key}: {value:.4f}\")\n",
    "\n",
    "# Save model\n",
    "model.save_pretrained('./experience_classifier')\n",
    "tokenizer.save_pretrained('./experience_classifier')\n",
    "print(\"\\n‚úÖ Model saved to: ./experience_classifier\")\n",
    "print(\"üì• Download and place in models/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ddbb51",
   "metadata": {},
   "source": [
    "## üìà Task 4: Test the Fine-tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3cab93e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Testing model predictions:\n",
      "\n",
      "1. Senior Software Engineer with 8 years of experience in Python, Java, and cloud t...\n",
      "   Predicted: SENIOR (confidence: 99.72%)\n",
      "\n",
      "2. Recent Computer Science graduate with internship experience. Proficient in Pytho...\n",
      "   Predicted: ENTRY (confidence: 99.48%)\n",
      "\n",
      "3. Distinguished architect with 15+ years building enterprise systems. Expert in sy...\n",
      "   Predicted: SENIOR (confidence: 99.74%)\n",
      "\n",
      "1. Senior Software Engineer with 8 years of experience in Python, Java, and cloud t...\n",
      "   Predicted: SENIOR (confidence: 99.72%)\n",
      "\n",
      "2. Recent Computer Science graduate with internship experience. Proficient in Pytho...\n",
      "   Predicted: ENTRY (confidence: 99.48%)\n",
      "\n",
      "3. Distinguished architect with 15+ years building enterprise systems. Expert in sy...\n",
      "   Predicted: SENIOR (confidence: 99.74%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test predictions\n",
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline('text-classification', model='./experience_classifier', tokenizer=tokenizer)\n",
    "\n",
    "test_resumes = [\n",
    "    \"Senior Software Engineer with 8 years of experience in Python, Java, and cloud technologies. Led teams of 5+ developers.\",\n",
    "    \"Recent Computer Science graduate with internship experience. Proficient in Python and JavaScript.\",\n",
    "    \"Distinguished architect with 15+ years building enterprise systems. Expert in system design and leadership.\"\n",
    "]\n",
    "\n",
    "print(\"üß™ Testing model predictions:\\n\")\n",
    "for i, text in enumerate(test_resumes, 1):\n",
    "    result = classifier(text[:512])[0]\n",
    "    predicted_label = reverse_label_map[int(result['label'].split('_')[-1])]\n",
    "    print(f\"{i}. {text[:80]}...\")\n",
    "    print(f\"   Predicted: {predicted_label.upper()} (confidence: {result['score']:.2%})\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f94892",
   "metadata": {},
   "source": [
    "## üì¶ Summary & Download Files\n",
    "\n",
    "Download these files and add to your local project:\n",
    "\n",
    "1. **resume_embeddings.npy** ‚Üí `models/embeddings/`\n",
    "2. **resume_faiss_index.bin** ‚Üí `models/embeddings/`\n",
    "3. **experience_classifier/** (folder) ‚Üí `models/`\n",
    "4. **resume_embeddings.json** (optional, backup)\n",
    "\n",
    "Then update your local code to use these GPU-trained models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c09c9ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: resume_embeddings.npy (172 bytes security) (deflated 8%)\n",
      "  adding: resume_faiss_index.bin (172 bytes security) (deflated 8%)\n",
      "  adding: experience_classifier/ (260 bytes security) (stored 0%)\n",
      "  adding: experience_classifier/config.json (172 bytes security) (deflated 52%)\n",
      "  adding: experience_classifier/model.safetensors (172 bytes security) (deflated 8%)\n",
      "  adding: experience_classifier/special_tokens_map.json (172 bytes security) (deflated 43%)\n",
      "  adding: experience_classifier/tokenizer.json (172 bytes security) (deflated 71%)\n",
      "  adding: experience_classifier/tokenizer_config.json (172 bytes security) (deflated 76%)\n",
      "  adding: experience_classifier/vocab.txt (172 bytes security) (deflated 53%)\n",
      "‚úÖ Created intellimatch_gpu_models.zip\n",
      "üì• Download using the file browser on the left\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Create zip for easy download\n",
    "!zip -r intellimatch_gpu_models.zip resume_embeddings.npy resume_faiss_index.bin experience_classifier/\n",
    "print(\"‚úÖ Created intellimatch_gpu_models.zip\")\n",
    "print(\"üì• Download using the file browser on the left\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
