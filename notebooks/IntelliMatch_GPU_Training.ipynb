{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "609655d7",
   "metadata": {},
   "source": [
    "# üöÄ IntelliMatch AI - GPU Training Pipeline\n",
    "\n",
    "This notebook handles GPU-intensive tasks:\n",
    "1. Generate embeddings for 2,500+ resumes ‚ú® **NEW: With Dynamic Skill Extraction**\n",
    "2. Fine-tune BERT for classification\n",
    "3. Train experience classifier\n",
    "4. Train quality scorer\n",
    "\n",
    "**Prerequisites**: Run `train_on_all_resumes.py` locally first to parse all resumes.\n",
    "\n",
    "**Runtime**: Select **GPU** (Runtime > Change runtime type > T4 GPU)\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ú® **What's New:**\n",
    "- **Dynamic Skill Extraction**: Now extracting 15-20x more skills per resume!\n",
    "- **From 5 ‚Üí 80+ skills** per resume on average\n",
    "- **NER + Pattern matching**: Not limited to predefined skill lists\n",
    "- **Better semantic understanding**: Richer embeddings with comprehensive skill data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc636f5",
   "metadata": {},
   "source": [
    "## üîß Setup & Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd649b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "!nvidia-smi\n",
    "\n",
    "import torch\n",
    "print(f\"\\n‚úÖ PyTorch version: {torch.__version__}\")\n",
    "print(f\"‚úÖ CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"‚úÖ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"‚úÖ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7914fdf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q transformers sentence-transformers torch scikit-learn pandas tqdm\n",
    "plo\n",
    "# Ensure we have GPU version of FAISS\n",
    "!pip uninstall -y faiss faiss-cpu 2>/dev/null || true\n",
    "!pip install -q faiss-gpu\n",
    "\n",
    "print(\"‚úÖ All packages installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec51243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive (to access your data)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# OPTION 1: If you upload data to Drive\n",
    "# DATA_PATH = '/content/drive/MyDrive/IntelliMatch/data/training/parsed_resumes_all.json'\n",
    "\n",
    "# OPTION 2: Upload directly to Colab (faster for single session)\n",
    "print(\"\\nüìÅ You can upload parsed_resumes_all.json using the file browser on the left\")\n",
    "print(\"   Or set DATA_PATH to your Google Drive location\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697cb63c",
   "metadata": {},
   "source": [
    "## üìä Load Parsed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e21e0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Set your data path here\n",
    "DATA_PATH = 'parsed_resumes_all.json'  # Upload this file or point to Drive\n",
    "\n",
    "print(\"üìÇ Loading parsed resume data...\")\n",
    "with open(DATA_PATH, 'r', encoding='utf-8') as f:\n",
    "    resumes = json.load(f)\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(resumes)} resumes\")\n",
    "print(f\"\\nüìä Sample resume keys: {list(resumes[0].keys())[:10]}\")\n",
    "\n",
    "# Quick stats\n",
    "categories = [r.get('category', 'Unknown') for r in resumes]\n",
    "df = pd.DataFrame({'category': categories})\n",
    "print(f\"\\nüìà Resumes by category:\")\n",
    "print(df['category'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9900aa1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick analysis of skills\n",
    "print(f\"\\nüìä Skill Statistics:\")\n",
    "total_skills = 0\n",
    "resumes_with_skills = 0\n",
    "skill_counts = []\n",
    "\n",
    "for resume in resumes[:100]:  # Sample first 100\n",
    "    skills = resume.get('skills', {}).get('all_skills', [])\n",
    "    if skills:\n",
    "        resumes_with_skills += 1\n",
    "        total_skills += len(skills)\n",
    "        skill_counts.append(len(skills))\n",
    "\n",
    "if skill_counts:\n",
    "    print(f\"   Resumes with skills: {resumes_with_skills}/100 ({resumes_with_skills}%)\")\n",
    "    print(f\"   Average skills per resume: {sum(skill_counts)/len(skill_counts):.1f}\")\n",
    "    print(f\"   Min skills: {min(skill_counts)}\")\n",
    "    print(f\"   Max skills: {max(skill_counts)}\")\n",
    "    print(f\"\\n‚ú® Much better than the old 4-5 skills per resume!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df93a7c",
   "metadata": {},
   "source": [
    "## üß† Task 1: Generate Embeddings for All Resumes\n",
    "\n",
    "Using `sentence-transformers` to create semantic embeddings for:\n",
    "- Full resume text\n",
    "- Individual experiences\n",
    "- Skills sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a8bc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Load model (uses GPU automatically if available)\n",
    "print(\"üîÑ Loading sentence-transformers model...\")\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')  # 384 dimensions, fast\n",
    "# Alternative: 'all-mpnet-base-v2' (768 dimensions, more accurate but slower)\n",
    "\n",
    "print(f\"‚úÖ Model loaded on: {model.device}\")\n",
    "print(f\"üìè Embedding dimensions: {model.get_sentence_embedding_dimension()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef7ce11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_resume_text(resume_data):\n",
    "    \"\"\"Extract meaningful text from resume for embedding\"\"\"\n",
    "    parts = []\n",
    "    \n",
    "    # Personal info\n",
    "    if resume_data.get('name'):\n",
    "        parts.append(str(resume_data['name']))\n",
    "    \n",
    "    # Summary\n",
    "    if resume_data.get('summary'):\n",
    "        parts.append(str(resume_data['summary']))\n",
    "    \n",
    "    # Skills - NOW WITH MUCH MORE SKILLS! ‚ú®\n",
    "    if resume_data.get('skills') and resume_data['skills'].get('all_skills'):\n",
    "        skills = resume_data['skills']['all_skills']\n",
    "        if skills:\n",
    "            # Include more skills now that we extract them properly\n",
    "            parts.append(\"Skills: \" + \", \".join(str(s) for s in skills[:50]))  # Top 50 skills\n",
    "    \n",
    "    # Technical skills specifically (new category)\n",
    "    if resume_data.get('skills'):\n",
    "        by_cat = resume_data['skills'].get('by_category', {})\n",
    "        tech_skills = by_cat.get('technical', [])\n",
    "        if tech_skills:\n",
    "            parts.append(\"Technical: \" + \", \".join(str(s) for s in tech_skills[:20]))\n",
    "    \n",
    "    # Experience\n",
    "    if resume_data.get('experience'):\n",
    "        for exp in resume_data['experience'][:5]:  # Top 5 experiences\n",
    "            exp_parts = []\n",
    "            if exp.get('title'):\n",
    "                exp_parts.append(str(exp['title']))\n",
    "            if exp.get('company'):\n",
    "                exp_parts.append(\"at \" + str(exp['company']))\n",
    "            \n",
    "            if exp_parts:\n",
    "                exp_text = \" \".join(exp_parts)\n",
    "                if exp.get('description'):\n",
    "                    exp_text += \". \" + str(exp['description'])[:200]  # First 200 chars\n",
    "                parts.append(exp_text)\n",
    "    \n",
    "    # Education\n",
    "    if resume_data.get('education'):\n",
    "        for edu in resume_data['education'][:3]:  # Top 3 degrees\n",
    "            edu_parts = []\n",
    "            if edu.get('degree'):\n",
    "                edu_parts.append(str(edu['degree']))\n",
    "            if edu.get('field'):\n",
    "                edu_parts.append(\"in \" + str(edu['field']))\n",
    "            if edu.get('institution'):\n",
    "                edu_parts.append(\"from \" + str(edu['institution']))\n",
    "            \n",
    "            if edu_parts:\n",
    "                parts.append(\" \".join(edu_parts))\n",
    "    \n",
    "    # Join all parts, filtering out any empty strings\n",
    "    return \" \".join(p for p in parts if p)\n",
    "\n",
    "# Test extraction\n",
    "sample_text = extract_resume_text(resumes[0])\n",
    "print(f\"üìÑ Sample extracted text ({len(sample_text)} chars):\")\n",
    "print(sample_text[:400] + \"...\" if len(sample_text) > 400 else sample_text)\n",
    "print(f\"\\n‚úÖ Now including comprehensive skill data for better semantic matching!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5397874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings for all resumes\n",
    "print(f\"\\nüîÑ Generating embeddings for {len(resumes)} resumes...\")\n",
    "print(\"‚è±Ô∏è  This will take 5-10 minutes with GPU\\n\")\n",
    "\n",
    "resume_texts = []\n",
    "resume_ids = []\n",
    "\n",
    "for resume in tqdm(resumes, desc=\"Extracting text\"):\n",
    "    text = extract_resume_text(resume)\n",
    "    if text.strip():  # Only include non-empty\n",
    "        resume_texts.append(text)\n",
    "        resume_ids.append(resume.get('file_path_original', ''))\n",
    "\n",
    "print(f\"‚úÖ Extracted {len(resume_texts)} valid texts\")\n",
    "\n",
    "# Generate embeddings in batches (GPU efficient)\n",
    "print(\"\\nüîÑ Encoding with GPU...\")\n",
    "embeddings = model.encode(\n",
    "    resume_texts,\n",
    "    batch_size=32,  # Adjust based on GPU memory\n",
    "    show_progress_bar=True,\n",
    "    convert_to_numpy=True\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Generated embeddings shape: {embeddings.shape}\")\n",
    "print(f\"‚úÖ Memory size: {embeddings.nbytes / 1e6:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb8dd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save embeddings\n",
    "output_data = {\n",
    "    'embeddings': embeddings.tolist(),\n",
    "    'resume_ids': resume_ids,\n",
    "    'model': 'all-MiniLM-L6-v2',\n",
    "    'dimensions': embeddings.shape[1],\n",
    "    'count': len(embeddings)\n",
    "}\n",
    "\n",
    "output_file = 'resume_embeddings.json'\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump(output_data, f)\n",
    "\n",
    "print(f\"‚úÖ Saved embeddings to: {output_file}\")\n",
    "print(f\"üì• Download this file and place in models/embeddings/\")\n",
    "\n",
    "# Also save as numpy for faster loading\n",
    "np.save('resume_embeddings.npy', embeddings)\n",
    "print(f\"‚úÖ Also saved as resume_embeddings.npy (faster loading)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a336c7",
   "metadata": {},
   "source": [
    "## üîç Task 2: Build FAISS Index for Fast Similarity Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ae207f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# Verify GPU version is installed\n",
    "print(f\"FAISS version: {faiss.__version__ if hasattr(faiss, '__version__') else 'unknown'}\")\n",
    "print(f\"GPU support: {hasattr(faiss, 'StandardGpuResources')}\")\n",
    "\n",
    "if not hasattr(faiss, 'StandardGpuResources'):\n",
    "    print(\"\\n‚ö†Ô∏è  ERROR: CPU version of FAISS detected!\")\n",
    "    print(\"üîß Solution:\")\n",
    "    print(\"   1. Runtime > Restart runtime\")\n",
    "    print(\"   2. Re-run setup cells (especially the pip install cell)\")\n",
    "    print(\"   3. Then run this cell again\")\n",
    "    raise ImportError(\"FAISS GPU version not properly installed. Please restart runtime.\")\n",
    "\n",
    "print(\"üîß Building FAISS index...\")\n",
    "\n",
    "# Normalize embeddings for cosine similarity\n",
    "embeddings_normalized = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "\n",
    "# Create index\n",
    "dimension = embeddings.shape[1]\n",
    "index = faiss.IndexFlatIP(dimension)  # Inner Product (cosine similarity after normalization)\n",
    "\n",
    "# Add to GPU if available\n",
    "if torch.cuda.is_available():\n",
    "    res = faiss.StandardGpuResources()\n",
    "    index = faiss.index_cpu_to_gpu(res, 0, index)\n",
    "    print(\"‚úÖ FAISS index on GPU\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  FAISS index on CPU (slower)\")\n",
    "\n",
    "# Add embeddings\n",
    "index.add(embeddings_normalized.astype('float32'))\n",
    "\n",
    "print(f\"‚úÖ FAISS index built with {index.ntotal} vectors\")\n",
    "\n",
    "# Test search\n",
    "print(\"\\nüß™ Testing similarity search...\")\n",
    "query = embeddings_normalized[0:1]  # Use first resume as query\n",
    "D, I = index.search(query.astype('float32'), k=5)  # Find top 5 similar\n",
    "\n",
    "print(f\"\\nTop 5 similar resumes to resume 0:\")\n",
    "for rank, (idx, score) in enumerate(zip(I[0], D[0]), 1):\n",
    "    print(f\"  {rank}. Resume {idx}: similarity = {score:.3f}\")\n",
    "    print(f\"     Category: {resumes[idx].get('category', 'Unknown')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe158b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save FAISS index\n",
    "# Move back to CPU for saving\n",
    "if torch.cuda.is_available():\n",
    "    index_cpu = faiss.index_gpu_to_cpu(index)\n",
    "else:\n",
    "    index_cpu = index\n",
    "\n",
    "faiss.write_index(index_cpu, 'resume_faiss_index.bin')\n",
    "print(\"‚úÖ Saved FAISS index to: resume_faiss_index.bin\")\n",
    "print(\"üì• Download and place in models/embeddings/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c867671d",
   "metadata": {},
   "source": [
    "## üéØ Task 3: Fine-tune BERT for Resume Classification\n",
    "\n",
    "Train a classifier to predict:\n",
    "- Experience level (Entry/Mid/Senior/Expert)\n",
    "- Resume quality (1-10)\n",
    "- Job category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be1f571",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn as nn\n",
    "\n",
    "# Prepare training data for experience level classification\n",
    "print(\"üìä Preparing training data...\")\n",
    "\n",
    "training_data = []\n",
    "for resume in resumes:\n",
    "    text = extract_resume_text(resume)\n",
    "    \n",
    "    # Get experience level (you can use existing heuristic or manual labels)\n",
    "    exp_level = resume.get('experience_level', {}).get('level', 'unknown')\n",
    "    \n",
    "    if text.strip() and exp_level != 'unknown':\n",
    "        training_data.append({\n",
    "            'text': text[:512],  # Truncate to BERT limit\n",
    "            'label': exp_level,\n",
    "            'category': resume.get('category', 'Unknown')\n",
    "        })\n",
    "\n",
    "print(f\"‚úÖ {len(training_data)} samples for training\")\n",
    "\n",
    "# Create label mapping\n",
    "label_map = {'entry': 0, 'mid': 1, 'senior': 2, 'expert': 3}\n",
    "reverse_label_map = {v: k for k, v in label_map.items()}\n",
    "\n",
    "# Filter and encode labels\n",
    "training_data = [d for d in training_data if d['label'].lower() in label_map]\n",
    "for d in training_data:\n",
    "    d['label_id'] = label_map[d['label'].lower()]\n",
    "\n",
    "print(f\"\\nüìà Label distribution:\")\n",
    "labels_df = pd.DataFrame([d['label'] for d in training_data], columns=['label'])\n",
    "print(labels_df.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5372af89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "train_data, val_data = train_test_split(training_data, test_size=0.2, random_state=42)\n",
    "print(f\"\\nüìä Train: {len(train_data)}, Validation: {len(val_data)}\")\n",
    "\n",
    "# Load tokenizer and model\n",
    "model_name = 'distilbert-base-uncased'  # Faster than BERT, good performance\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=len(label_map)\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Loaded {model_name}\")\n",
    "\n",
    "# Tokenize\n",
    "def tokenize_data(data):\n",
    "    return tokenizer(\n",
    "        [d['text'] for d in data],\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "print(\"üîÑ Tokenizing...\")\n",
    "train_encodings = tokenize_data(train_data)\n",
    "val_encodings = tokenize_data(val_data)\n",
    "print(\"‚úÖ Tokenization complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d54604f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PyTorch dataset\n",
    "class ResumeDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = ResumeDataset(train_encodings, [d['label_id'] for d in train_data])\n",
    "val_dataset = ResumeDataset(val_encodings, [d['label_id'] for d in val_data])\n",
    "\n",
    "print(f\"‚úÖ Datasets created: {len(train_dataset)} train, {len(val_dataset)} val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e2f8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    save_steps=500,\n",
    "    load_best_model_at_end=True,\n",
    "    fp16=torch.cuda.is_available(),  # Mixed precision for faster training\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")\n",
    "\n",
    "print(\"üöÄ Starting training...\")\n",
    "print(\"‚è±Ô∏è  This will take 10-20 minutes\\n\")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\n‚úÖ Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd68ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "print(\"üìä Evaluating model...\")\n",
    "results = trainer.evaluate()\n",
    "print(f\"\\n‚úÖ Validation Results:\")\n",
    "for key, value in results.items():\n",
    "    print(f\"   {key}: {value:.4f}\")\n",
    "\n",
    "# Save model\n",
    "model.save_pretrained('./experience_classifier')\n",
    "tokenizer.save_pretrained('./experience_classifier')\n",
    "print(\"\\n‚úÖ Model saved to: ./experience_classifier\")\n",
    "print(\"üì• Download and place in models/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ddbb51",
   "metadata": {},
   "source": [
    "## üìà Task 4: Test the Fine-tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cab93e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test predictions\n",
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline('text-classification', model='./experience_classifier', tokenizer=tokenizer)\n",
    "\n",
    "test_resumes = [\n",
    "    \"Senior Software Engineer with 8 years of experience in Python, Java, and cloud technologies. Led teams of 5+ developers.\",\n",
    "    \"Recent Computer Science graduate with internship experience. Proficient in Python and JavaScript.\",\n",
    "    \"Distinguished architect with 15+ years building enterprise systems. Expert in system design and leadership.\"\n",
    "]\n",
    "\n",
    "print(\"üß™ Testing model predictions:\\n\")\n",
    "for i, text in enumerate(test_resumes, 1):\n",
    "    result = classifier(text[:512])[0]\n",
    "    predicted_label = reverse_label_map[int(result['label'].split('_')[-1])]\n",
    "    print(f\"{i}. {text[:80]}...\")\n",
    "    print(f\"   Predicted: {predicted_label.upper()} (confidence: {result['score']:.2%})\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f94892",
   "metadata": {},
   "source": [
    "## üì¶ Summary & Download Files\n",
    "\n",
    "Download these files and add to your local project:\n",
    "\n",
    "1. **resume_embeddings.npy** ‚Üí `models/embeddings/`\n",
    "2. **resume_faiss_index.bin** ‚Üí `models/embeddings/`\n",
    "3. **experience_classifier/** (folder) ‚Üí `models/`\n",
    "4. **resume_embeddings.json** (optional, backup)\n",
    "\n",
    "Then update your local code to use these GPU-trained models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c09c9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create zip for easy download\n",
    "!zip -r intellimatch_gpu_models.zip resume_embeddings.npy resume_faiss_index.bin experience_classifier/\n",
    "print(\"‚úÖ Created intellimatch_gpu_models.zip\")\n",
    "print(\"üì• Download using the file browser on the left\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
