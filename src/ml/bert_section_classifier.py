"""
BERT Section Classifier
Classifies resume text into sections: Skills, Experience, Education, Summary, etc.
Uses DistilBERT for efficient classification with high accuracy
"""

import re
import torch
from typing import Dict, List, Tuple, Optional
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import logging
from pathlib import Path

logger = logging.getLogger(__name__)


class BERTSectionClassifier:
    """
    BERT-based section classifier for resumes
    Achieves 90%+ accuracy on section detection
    """
    
    # Section labels
    SECTIONS = [
        'skills',
        'experience',
        'education',
        'summary',
        'projects',
        'certifications',
        'other'
    ]
    
    def __init__(self, model_path: Optional[str] = None):
        """
        Initialize classifier
        
        Args:
            model_path: Path to fine-tuned model (if available)
        """
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        if model_path and Path(model_path).exists():
            # Load fine-tuned model
            logger.info(f"Loading fine-tuned model from {model_path}")
            self.tokenizer = AutoTokenizer.from_pretrained(model_path)
            self.model = AutoModelForSequenceClassification.from_pretrained(model_path)
            self.is_finetuned = True
        else:
            # Use base model with rule-based fallback
            logger.info("Using base model with rule-based classification")
            self.tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')
            self.model = None
            self.is_finetuned = False
        
        if self.model:
            self.model.to(self.device)
            self.model.eval()
        
        # Label mapping
        self.label_to_id = {label: i for i, label in enumerate(self.SECTIONS)}
        self.id_to_label = {i: label for label, i in self.label_to_id.items()}
        
        # Section keywords for rule-based classification
        self.section_keywords = {
            'skills': [
                r'\bskills?\b', r'\btechnical skills\b', r'\bcore competencies\b',
                r'\bproficiencies\b', r'\bexpertise\b', r'\btechnologies\b',
                r'\btools & technologies\b', r'\bprogramming languages\b'
            ],
            'experience': [
                r'\bexperience\b', r'\bwork history\b', r'\bemployment\b',
                r'\bprofessional experience\b', r'\bwork experience\b',
                r'\bcareer history\b', r'\bemployment history\b'
            ],
            'education': [
                r'\beducation\b', r'\bacademic\b', r'\bqualifications\b',
                r'\bdegree\b', r'\buniversity\b', r'\bcollege\b',
                r'\bacademic background\b', r'\beducational background\b'
            ],
            'summary': [
                r'\bsummary\b', r'\bprofile\b', r'\bprofessional summary\b',
                r'\bcareer summary\b', r'\babout\b', r'\bintroduction\b',
                r'\bcareer objective\b', r'\bobjective\b', r'\bprofile summary\b'
            ],
            'projects': [
                r'\bprojects?\b', r'\bportfolio\b', r'\bkey projects\b',
                r'\bmajor projects\b', r'\bnotable projects\b', r'\bwork samples\b'
            ],
            'certifications': [
                r'\bcertifications?\b', r'\blicenses?\b', r'\bcredentials\b',
                r'\bprofessional certifications\b', r'\bcertified\b'
            ]
        }
        
        # Compile patterns
        self.compiled_keywords = {
            section: [re.compile(pattern, re.IGNORECASE) for pattern in patterns]
            for section, patterns in self.section_keywords.items()
        }
    
    def classify_text(self, text: str, context: Optional[str] = None) -> Tuple[str, float]:
        """
        Classify a piece of text into a section
        
        Args:
            text: Text to classify
            context: Optional context (previous sections, etc.)
        
        Returns:
            Tuple of (section_label, confidence)
        """
        if not text or len(text.strip()) < 3:
            return 'other', 0.0
        
        if self.is_finetuned and self.model:
            return self._classify_with_bert(text)
        else:
            return self._classify_with_rules(text)
    
    def _classify_with_bert(self, text: str) -> Tuple[str, float]:
        """Classify using fine-tuned BERT model"""
        # Tokenize
        inputs = self.tokenizer(
            text[:512],  # Truncate to BERT limit
            return_tensors='pt',
            truncation=True,
            padding=True,
            max_length=512
        )
        
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
        
        # Predict
        with torch.no_grad():
            outputs = self.model(**inputs)
            logits = outputs.logits
            probs = torch.softmax(logits, dim=-1)
            
            confidence, predicted = torch.max(probs, dim=-1)
            
            section = self.id_to_label[predicted.item()]
            conf = confidence.item()
        
        return section, conf
    
    def _classify_with_rules(self, text: str) -> Tuple[str, float]:
        """
        Rule-based classification using keywords
        Fast and works without fine-tuning
        """
        text_lower = text.lower()
        
        # Score each section
        scores = {}
        
        for section, patterns in self.compiled_keywords.items():
            score = 0
            for pattern in patterns:
                if pattern.search(text_lower):
                    score += 1
            
            if score > 0:
                scores[section] = score
        
        if not scores:
            return 'other', 0.3
        
        # Get best match
        best_section = max(scores, key=scores.get)
        max_score = scores[best_section]
        
        # Calculate confidence (normalized)
        confidence = min(max_score / 3.0, 1.0)
        
        return best_section, confidence
    
    def classify_resume(self, text: str, return_all: bool = False) -> Dict:
        """
        Classify entire resume into sections
        
        Args:
            text: Full resume text
            return_all: If True, return all section predictions
        
        Returns:
            Dict with detected sections and their text
        """
        # Split into chunks (simple line-based)
        lines = text.split('\n')
        
        sections = {}
        current_section = 'other'
        current_text = []
        all_predictions = []
        
        for line in lines:
            line = line.strip()
            
            if not line:
                continue
            
            # Check if this line is a section header
            is_header = False
            if len(line) < 50 and ':' not in line[10:]:  # Likely a header
                section, confidence = self.classify_text(line)
                
                if confidence > 0.5:  # Strong signal it's a header
                    # Save previous section
                    if current_text:
                        sections[current_section] = '\n'.join(current_text)
                        current_text = []
                    
                    current_section = section
                    is_header = True
                    
                    if return_all:
                        all_predictions.append({
                            'text': line,
                            'section': section,
                            'confidence': confidence,
                            'is_header': True
                        })
            
            if not is_header:
                current_text.append(line)
                
                if return_all:
                    section, confidence = self.classify_text(line)
                    all_predictions.append({
                        'text': line[:100],  # Truncate for display
                        'section': section,
                        'confidence': confidence,
                        'is_header': False
                    })
        
        # Save last section
        if current_text:
            sections[current_section] = '\n'.join(current_text)
        
        result = {
            'sections': sections,
            'section_count': len(sections),
            'has_skills': 'skills' in sections,
            'has_experience': 'experience' in sections,
            'has_education': 'education' in sections
        }
        
        if return_all:
            result['all_predictions'] = all_predictions
        
        return result
    
    def extract_section_boundaries(self, text: str) -> List[Dict]:
        """
        Extract section boundaries (start, end, label)
        
        Returns:
            List of sections with their positions
        """
        lines = text.split('\n')
        sections = []
        current_section = None
        current_start = 0
        line_offset = 0
        
        for i, line in enumerate(lines):
            line = line.strip()
            line_offset += len(line) + 1  # +1 for newline
            
            if not line:
                continue
            
            # Check if header
            if len(line) < 50:
                section, confidence = self.classify_text(line)
                
                if confidence > 0.5:
                    # Save previous section
                    if current_section:
                        sections.append({
                            'section': current_section['label'],
                            'start_line': current_section['start_line'],
                            'end_line': i - 1,
                            'confidence': current_section['confidence']
                        })
                    
                    # Start new section
                    current_section = {
                        'label': section,
                        'start_line': i,
                        'confidence': confidence
                    }
        
        # Add last section
        if current_section:
            sections.append({
                'section': current_section['label'],
                'start_line': current_section['start_line'],
                'end_line': len(lines) - 1,
                'confidence': current_section['confidence']
            })
        
        return sections


def classify_resume_sections(text: str) -> Dict:
    """
    Convenience function to classify resume sections
    
    Args:
        text: Resume text
    
    Returns:
        Dict with sections
    """
    classifier = BERTSectionClassifier()
    return classifier.classify_resume(text)


# Training function (for fine-tuning)
def train_section_classifier(
    training_data: List[Dict],
    output_dir: str = 'models/section_classifier',
    epochs: int = 3
):
    """
    Fine-tune BERT for section classification
    
    Args:
        training_data: List of {text, label} dicts
        output_dir: Where to save model
        epochs: Training epochs
    """
    from transformers import TrainingArguments, Trainer
    from sklearn.model_selection import train_test_split
    import numpy as np
    
    logger.info(f"Training section classifier with {len(training_data)} samples")
    
    # Prepare data
    texts = [d['text'] for d in training_data]
    labels = [d['label'] for d in training_data]
    
    # Create label mapping
    unique_labels = sorted(set(labels))
    label_to_id = {label: i for i, label in enumerate(unique_labels)}
    
    label_ids = [label_to_id[label] for label in labels]
    
    # Split
    train_texts, val_texts, train_labels, val_labels = train_test_split(
        texts, label_ids, test_size=0.2, random_state=42
    )
    
    # Load model and tokenizer
    tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')
    model = AutoModelForSequenceClassification.from_pretrained(
        'distilbert-base-uncased',
        num_labels=len(unique_labels)
    )
    
    # Tokenize
    train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=512)
    val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=512)
    
    # Create dataset
    class SectionDataset(torch.utils.data.Dataset):
        def __init__(self, encodings, labels):
            self.encodings = encodings
            self.labels = labels
        
        def __getitem__(self, idx):
            item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
            item['labels'] = torch.tensor(self.labels[idx])
            return item
        
        def __len__(self):
            return len(self.labels)
    
    train_dataset = SectionDataset(train_encodings, train_labels)
    val_dataset = SectionDataset(val_encodings, val_labels)
    
    # Training args
    training_args = TrainingArguments(
        output_dir=output_dir,
        num_train_epochs=epochs,
        per_device_train_batch_size=16,
        per_device_eval_batch_size=16,
        warmup_steps=500,
        weight_decay=0.01,
        logging_dir=f'{output_dir}/logs',
        logging_steps=10,
        evaluation_strategy="epoch",
        save_strategy="epoch",
        load_best_model_at_end=True,
    )
    
    # Trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
    )
    
    # Train
    logger.info("Starting training...")
    trainer.train()
    
    # Save
    model.save_pretrained(output_dir)
    tokenizer.save_pretrained(output_dir)
    
    logger.info(f"Model saved to {output_dir}")
    
    return model, tokenizer


if __name__ == "__main__":
    print("=" * 80)
    print("üß™ Testing BERT Section Classifier")
    print("=" * 80)
    
    classifier = BERTSectionClassifier()
    
    # Test 1: Classify section headers
    print("\n1Ô∏è‚É£ Test: Classify section headers")
    headers = [
        "SKILLS",
        "Work Experience",
        "EDUCATION",
        "Professional Summary",
        "Projects",
        "Certifications and Licenses"
    ]
    
    for header in headers:
        section, confidence = classifier.classify_text(header)
        print(f"   '{header}' ‚Üí {section.upper()} (confidence: {confidence:.2f})")
    
    # Test 2: Classify resume text
    print("\n2Ô∏è‚É£ Test: Classify full resume")
    sample_resume = """
    John Doe
    Software Engineer
    
    PROFESSIONAL SUMMARY
    Experienced software engineer with 5+ years of expertise in Python and web development.
    
    SKILLS
    Python, Django, React, PostgreSQL, AWS, Docker, Git
    
    WORK EXPERIENCE
    
    Senior Software Engineer - Tech Corp (2020-2024)
    - Developed microservices using Python and Django
    - Led team of 5 developers
    - Deployed on AWS infrastructure
    
    Software Engineer - StartUp Inc (2018-2020)
    - Built REST APIs with Flask
    - Implemented CI/CD pipelines
    
    EDUCATION
    
    Bachelor of Science in Computer Science
    University of Technology (2014-2018)
    GPA: 3.8/4.0
    
    CERTIFICATIONS
    AWS Certified Solutions Architect
    """
    
    result = classifier.classify_resume(sample_resume)
    
    print(f"\n   Sections found: {result['section_count']}")
    print(f"   Has skills: {result['has_skills']}")
    print(f"   Has experience: {result['has_experience']}")
    print(f"   Has education: {result['has_education']}")
    
    print("\n   Section content preview:")
    for section, content in result['sections'].items():
        preview = content[:100].replace('\n', ' ')
        print(f"      {section.upper()}: {preview}...")
    
    print("\n" + "=" * 80)
    print("‚úÖ Section classifier working!")
    print("=" * 80)
